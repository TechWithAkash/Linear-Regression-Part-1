{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05b4464b",
   "metadata": {},
   "source": [
    "Q1. Explain the difference between simple linear regression and multiple linear regression. Provide an\n",
    "example of each."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b68bb13",
   "metadata": {},
   "source": [
    "=>\n",
    "Simple linear regression and multiple linear regression are both types of linear regression models used in statistics and machine learning. They have similarities, but the key difference is in the number of independent variables (features) they use to make predictions. Let's discuss each and provide examples for both.\n",
    "\n",
    "1. **Simple Linear Regression**:\n",
    "\n",
    "   - **Definition**: Simple linear regression models the relationship between a dependent variable (target) and a single independent variable (feature).\n",
    "   - **Equation**: The equation for simple linear regression is often represented as:\n",
    "     ```\n",
    "     y = mx + b\n",
    "     ```\n",
    "     Where:\n",
    "     - `y` is the dependent variable.\n",
    "     - `x` is the independent variable.\n",
    "     - `m` is the slope (the coefficient that represents the effect of the independent variable on the dependent variable).\n",
    "     - `b` is the y-intercept (the point where the regression line crosses the y-axis).\n",
    "\n",
    "   - **Example**:\n",
    "     Let's say you want to predict a person's weight (`y`) based on their height (`x`). You collect data on the heights and weights of several individuals. In this case, you are using a simple linear regression model because you are using only one feature (height) to predict weight. The equation might look like:\n",
    "     ```\n",
    "     Weight = (Slope) * Height + (Intercept)\n",
    "     ```\n",
    "\n",
    "2. **Multiple Linear Regression**:\n",
    "\n",
    "   - **Definition**: Multiple linear regression models the relationship between a dependent variable (target) and two or more independent variables (features).\n",
    "   - **Equation**: The equation for multiple linear regression can be represented as:\n",
    "     ```\n",
    "     y = b0 + b1*x1 + b2*x2 + ... + bn*xn\n",
    "     ```\n",
    "     Where:\n",
    "     - `y` is the dependent variable.\n",
    "     - `x1`, `x2`, ..., `xn` are the independent variables.\n",
    "     - `b0` is the y-intercept.\n",
    "     - `b1`, `b2`, ..., `bn` are the coefficients that represent the effect of each independent variable on the dependent variable.\n",
    "\n",
    "   - **Example**:\n",
    "     Suppose you want to predict a car's fuel efficiency (`y`) based on its engine size (`x1`), weight (`x2`), and horsepower (`x3`). In this scenario, you are using multiple linear regression because you are using three features to predict fuel efficiency. The equation might look like:\n",
    "     ```\n",
    "     Fuel Efficiency = (Intercept) + (Coefficient1) * Engine Size + (Coefficient2) * Weight + (Coefficient3) * Horsepower\n",
    "     ```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c361ec6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "55f7be55",
   "metadata": {},
   "source": [
    "Q2. Discuss the assumptions of linear regression. How can you check whether these assumptions hold in\n",
    "a given dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91111315",
   "metadata": {},
   "source": [
    "=>\n",
    "Linear regression relies on several assumptions about the data to be valid and produce reliable results. It's essential to check these assumptions to ensure the appropriateness of using linear regression for a given dataset. The primary assumptions of linear regression are as follows:\n",
    "\n",
    "1. **Linearity**: The relationship between the independent variables and the dependent variable is linear. This means that changes in the independent variables result in proportional changes in the dependent variable. You can check this assumption by creating scatterplots and visually inspecting the data. If the points form a roughly linear pattern, the linearity assumption is likely met.\n",
    "\n",
    "2. **Independence of Errors**: The errors (residuals) should be independent of each other. In other words, the error for one data point should not depend on the error for another data point. You can check this assumption by examining residual plots or using statistical tests like the Durbin-Watson test.\n",
    "\n",
    "3. **Homoscedasticity**: The variance of the errors should be constant across all levels of the independent variables. This means that the spread of residuals should be roughly consistent along the entire range of predicted values. You can check this assumption by creating scatterplots of residuals versus predicted values. If you see a \"fan\" shape, it may indicate heteroscedasticity, which violates this assumption.\n",
    "\n",
    "4. **Normality of Residuals**: The residuals should be normally distributed. In other words, the distribution of errors should resemble a bell-shaped curve. You can check this assumption by creating a histogram or a normal probability plot of residuals. You can also use statistical tests like the Shapiro-Wilk test or the Anderson-Darling test to assess normality.\n",
    "\n",
    "5. **No or Little Multicollinearity**: Multicollinearity occurs when independent variables are highly correlated with each other, making it challenging to determine their individual effects on the dependent variable. You can check for multicollinearity using correlation matrices or variance inflation factors (VIF) for each independent variable. VIF values greater than 1 may indicate multicollinearity.\n",
    "\n",
    "6. **No Endogeneity**: Endogeneity implies that the independent variables are correlated with the error term. It can be challenging to detect this assumption directly from the data, but theoretical knowledge of the problem domain can help identify potential sources of endogeneity.\n",
    "\n",
    "To check whether these assumptions hold in a given dataset, you can use various techniques and diagnostic tools:\n",
    "\n",
    "- Visual inspection of scatterplots, residual plots, and histograms.\n",
    "- Statistical tests like the Durbin-Watson test, Shapiro-Wilk test, Anderson-Darling test, and others.\n",
    "- Variance inflation factors (VIF) to check for multicollinearity.\n",
    "- Theoretical understanding of the data and problem domain to identify sources of endogeneity.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94973866",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "71bd012c",
   "metadata": {},
   "source": [
    "Q3. How do you interpret the slope and intercept in a linear regression model? Provide an example using\n",
    "a real-world scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47a7108",
   "metadata": {},
   "source": [
    "=>\n",
    "In a linear regression model, you have the equation of the form:\n",
    "\n",
    "\\[ \\text{Dependent Variable} = \\text{Intercept} + \\text{Slope} \\times \\text{Independent Variable} + \\text{Error} \\]\n",
    "\n",
    "Here's how you interpret the slope and intercept:\n",
    "\n",
    "1. **Intercept (b0)**: The intercept represents the predicted value of the dependent variable when the independent variable(s) are equal to zero. In some cases, this interpretation may not be meaningful, depending on the context. The intercept is also known as the baseline or the constant.\n",
    "\n",
    "2. **Slope (b1, b2, etc.)**: The slope represents the change in the dependent variable for a one-unit change in the independent variable while holding all other variables constant. In other words, it indicates the rate of change or the impact of the independent variable on the dependent variable.\n",
    "\n",
    "Now, let's provide an example using a real-world scenario:\n",
    "\n",
    "**Scenario**: Imagine you are conducting a study to understand the relationship between years of experience and salary for employees in a company. You collect data from a sample of employees, and you fit a simple linear regression model:\n",
    "\n",
    "\\[ \\text{Salary} = \\beta_0 + \\beta_1 \\times \\text{Years of Experience} + \\text{Error} \\]\n",
    "\n",
    "- **Intercept (\\(\\beta_0\\))**: In this context, the intercept represents the expected starting salary for an employee with zero years of experience. It's the baseline salary that someone might receive as a new hire.\n",
    "\n",
    "- **Slope (\\(\\beta_1\\))**: The slope represents the rate of change in salary for each additional year of experience. If \\(\\beta_1\\) is, for example, $5,000, it means that, on average, for each additional year of experience, an employee can expect their salary to increase by $5,000, assuming all other factors are constant.\n",
    "\n",
    "So, if the estimated intercept (\\(\\beta_0\\)) is $40,000 and the estimated slope (\\(\\beta_1\\)) is $5,000, you can interpret it as follows:\n",
    "\n",
    "- An employee with zero years of experience (a new hire) can expect to start with a salary of $40,000.\n",
    "- For each additional year of experience, an employee can expect their salary to increase by $5,000, assuming other factors like job role or location remain constant.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e242e3d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "269c630b",
   "metadata": {},
   "source": [
    "Q4. Explain the concept of gradient descent. How is it used in machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4c2b45",
   "metadata": {},
   "source": [
    "=>\n",
    "**Gradient descent** is an optimization algorithm used in machine learning and deep learning to find the minimum of a cost or loss function. It's a fundamental technique for training models by iteratively adjusting their parameters (weights or coefficients) to minimize the error between predicted and actual values. Here's an explanation of how gradient descent works and its role in machine learning:\n",
    "\n",
    "1. **The Objective Function**: In machine learning, you typically have a model with some parameters (weights) and a cost or loss function that measures how well the model is performing. The goal is to find the values of the model's parameters that minimize this cost function.\n",
    "\n",
    "2. **Initialization**: Gradient descent starts with an initial guess for the parameter values. These initial values can be random, zero, or some other suitable choice.\n",
    "\n",
    "3. **Gradient Calculation**: At each iteration, gradient descent calculates the gradient of the cost function with respect to the model's parameters. The gradient is a vector that points in the direction of the steepest increase in the cost function. Mathematically, it is the partial derivatives of the cost function with respect to each parameter.\n",
    "\n",
    "4. **Parameter Update**: Gradient descent adjusts the model's parameters by moving them in the opposite direction of the gradient. The size of this update is controlled by a hyperparameter called the learning rate (often denoted as Î±). The update rule for a single parameter looks like this:\n",
    "\n",
    "   \\[ \\text{New Parameter} = \\text{Old Parameter} - \\alpha \\times \\text{Gradient} \\]\n",
    "\n",
    "   This process is repeated for all parameters until convergence is achieved.\n",
    "\n",
    "5. **Convergence**: The algorithm continues iterating until it reaches a stopping criterion, such as a predefined number of iterations or when the change in the cost function becomes small enough.\n",
    "\n",
    "6. **Minimum Found**: When the algorithm converges, it has hopefully found parameter values that correspond to the minimum of the cost function, meaning the model fits the data well.\n",
    "\n",
    "**How Gradient Descent is Used in Machine Learning**:\n",
    "\n",
    "Gradient descent is a core optimization algorithm in machine learning for the following reasons:\n",
    "\n",
    "1. **Model Training**: It is the primary method used for training machine learning models. Whether it's linear regression, logistic regression, neural networks, or other models, gradient descent is used to find the optimal model parameters that minimize the prediction error.\n",
    "\n",
    "2. **Deep Learning**: In deep learning, especially in training neural networks with many parameters, gradient descent variants like stochastic gradient descent (SGD) and mini-batch gradient descent are crucial. These methods enable the training of deep models, and they are used in conjunction with backpropagation to adjust the neural network's weights.\n",
    "\n",
    "3. **Hyperparameter Tuning**: Gradient descent is also used for tuning hyperparameters, including the learning rate, to optimize the model's performance.\n",
    "\n",
    "4. **Feature Selection**: In feature selection algorithms, gradient descent can be employed to identify the most relevant features by minimizing a cost function that depends on the feature set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37e6d24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "64ba8482",
   "metadata": {},
   "source": [
    "Q5. Describe the multiple linear regression model. How does it differ from simple linear regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74adaa1c",
   "metadata": {},
   "source": [
    "=>\n",
    "**Multiple Linear Regression** is an extension of the simple linear regression model. While simple linear regression involves a single independent variable (predictor), multiple linear regression deals with two or more independent variables. In multiple linear regression, you aim to model the relationship between a dependent variable and multiple predictors. Here's a description of the multiple linear regression model and how it differs from simple linear regression:\n",
    "\n",
    "**Multiple Linear Regression Model**:\n",
    "\n",
    "The multiple linear regression model can be represented by the following equation:\n",
    "\n",
    "\\[ Y = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\ldots + \\beta_pX_p + \\varepsilon \\]\n",
    "\n",
    "- \\(Y\\) is the dependent variable (the variable you want to predict).\n",
    "- \\(\\beta_0\\) is the y-intercept (the value of \\(Y\\) when all predictors are zero).\n",
    "- \\(\\beta_1, \\beta_2, \\ldots, \\beta_p\\) are the coefficients for the independent variables \\(X_1, X_2, \\ldots, X_p\\), representing the effect of each predictor on \\(Y\\) while holding other predictors constant.\n",
    "- \\(X_1, X_2, \\ldots, X_p\\) are the independent variables (predictors) used in the model.\n",
    "- \\(\\varepsilon\\) represents the error term, which accounts for the variability not explained by the predictors. It's assumed to be normally distributed with a mean of 0.\n",
    "\n",
    "**Differences between Multiple and Simple Linear Regression**:\n",
    "\n",
    "1. **Number of Predictors**:\n",
    "   - Simple Linear Regression: In simple linear regression, there is only one independent variable.\n",
    "   - Multiple Linear Regression: In multiple linear regression, there are two or more independent variables.\n",
    "\n",
    "2. **Model Complexity**:\n",
    "   - Simple Linear Regression: It models a linear relationship between the dependent variable and a single independent variable.\n",
    "   - Multiple Linear Regression: It models a linear relationship between the dependent variable and multiple independent variables, allowing for more complex relationships to be captured.\n",
    "\n",
    "3. **Interpretation**:\n",
    "   - Simple Linear Regression: The interpretation of the slope coefficient is straightforward and relates to the change in the dependent variable for a one-unit change in the single independent variable.\n",
    "   - Multiple Linear Regression: The interpretation of the coefficients is more nuanced. Each coefficient represents the change in the dependent variable for a one-unit change in the corresponding independent variable, while holding all other predictors constant. It accounts for the potential interactions and interdependencies among the predictors.\n",
    "\n",
    "4. **Use Cases**:\n",
    "   - Simple Linear Regression: It's suitable for scenarios where there's a clear linear relationship between the dependent variable and a single predictor.\n",
    "   - Multiple Linear Regression: It's used when the dependent variable depends on multiple predictors or when you want to consider the combined influence of several factors on the outcome.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7309fa99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "97f5fc66",
   "metadata": {},
   "source": [
    "Q6. Explain the concept of multicollinearity in multiple linear regression. How can you detect and\n",
    "address this issue?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c232859",
   "metadata": {},
   "source": [
    "=>\n",
    "**Multicollinearity** is a statistical issue that occurs in multiple linear regression when two or more independent variables in the model are highly correlated. It can create problems when trying to understand the individual effect of each independent variable on the dependent variable because the presence of multicollinearity makes it challenging to isolate their unique contributions. Here's an explanation of multicollinearity and how to detect and address it:\n",
    "\n",
    "**Concept of Multicollinearity**:\n",
    "\n",
    "1. **High Correlation**: Multicollinearity arises when there are high correlations between two or more independent variables in a multiple linear regression model. In other words, one independent variable can be predicted from the others with a high degree of accuracy.\n",
    "\n",
    "2. **Impacts Interpretability**: Multicollinearity can make it difficult to discern the separate effects of correlated variables on the dependent variable. It can lead to unstable coefficient estimates, wide confidence intervals, and potentially misleading interpretations.\n",
    "\n",
    "**Detecting Multicollinearity**:\n",
    "\n",
    "There are several methods to detect multicollinearity:\n",
    "\n",
    "1. **Correlation Matrix**: Calculate the correlation coefficients between pairs of independent variables. If you find high absolute values of correlation coefficients (e.g., > 0.7 or 0.8), it suggests multicollinearity.\n",
    "\n",
    "2. **Variance Inflation Factor (VIF)**: Calculate the VIF for each independent variable. VIF measures how much the variance of an estimated regression coefficient is increased due to multicollinearity. A VIF greater than 1 indicates the presence of multicollinearity. Generally, VIF values above 5 or 10 are considered problematic.\n",
    "\n",
    "**Addressing Multicollinearity**:\n",
    "\n",
    "Once multicollinearity is detected, you can take several steps to address the issue:\n",
    "\n",
    "1. **Remove or Combine Variables**: One approach is to remove one of the correlated variables from the model if it's not crucial for your analysis. Alternatively, you can create a composite variable by combining the correlated variables.\n",
    "\n",
    "2. **Collect More Data**: Sometimes multicollinearity can be a result of a small dataset. Collecting more data can reduce the impact of multicollinearity.\n",
    "\n",
    "3. **Feature Selection**: Use feature selection techniques to identify and retain only the most important independent variables, discarding the redundant ones.\n",
    "\n",
    "4. **Principal Component Analysis (PCA)**: PCA is a dimensionality reduction technique that can be applied to address multicollinearity. It transforms the original variables into a set of uncorrelated variables (principal components) while retaining most of the information.\n",
    "\n",
    "5. **Regularization**: Techniques like Ridge and Lasso regression can be used to mitigate multicollinearity. These methods add a penalty to the regression coefficients, reducing their magnitude and, in turn, mitigating multicollinearity.\n",
    "\n",
    "6. **Partial Correlations**: Instead of examining the simple correlations, you can analyze partial correlations, which assess the relationships between variables while controlling for the effects of other variables.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a92571b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f05cd360",
   "metadata": {},
   "source": [
    "Q7. Describe the polynomial regression model. How is it different from linear regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfe589b",
   "metadata": {},
   "source": [
    "=>\n",
    "**Polynomial regression** is a type of regression analysis used in statistics and machine learning to model the relationship between the dependent variable and one or more independent variables. It differs from linear regression in that it allows for more complex, nonlinear relationships between the variables. Specifically, polynomial regression models use polynomial functions to capture the underlying patterns in the data.\n",
    "\n",
    "Here's a description of the polynomial regression model and how it differs from linear regression:\n",
    "\n",
    "**Polynomial Regression Model**:\n",
    "\n",
    "In a polynomial regression model, the relationship between the dependent variable (\\(Y\\)) and the independent variable (\\(X\\)) is expressed as a polynomial function. The most common form is a simple quadratic polynomial, but higher-order polynomials can also be used for more complex relationships. The quadratic form looks like this:\n",
    "\n",
    "\\[Y = \\beta_0 + \\beta_1X + \\beta_2X^2 + \\varepsilon\\]\n",
    "\n",
    "- \\(Y\\) is the dependent variable.\n",
    "- \\(\\beta_0\\) is the y-intercept.\n",
    "- \\(\\beta_1\\) is the coefficient for the linear term (\\(X\\)).\n",
    "- \\(\\beta_2\\) is the coefficient for the quadratic term (\\(X^2\\)).\n",
    "- \\(\\varepsilon\\) represents the error term.\n",
    "\n",
    "The quadratic term (\\(X^2\\)) allows for curvature in the relationship between \\(X\\) and \\(Y), capturing situations where the relationship is not a straight line but a curve.\n",
    "\n",
    "**Differences Between Polynomial and Linear Regression**:\n",
    "\n",
    "1. **Linearity**:\n",
    "   - Linear Regression: Assumes a linear relationship between the independent variable and the dependent variable. The relationship is modeled as a straight line.\n",
    "   - Polynomial Regression: Allows for nonlinear relationships, capturing more complex patterns by introducing polynomial terms (e.g., \\(X^2\\), \\(X^3\\)).\n",
    "\n",
    "2. **Model Complexity**:\n",
    "   - Linear Regression: Simpler model with fewer parameters. Suitable for linear relationships.\n",
    "   - Polynomial Regression: More complex model with additional parameters for each polynomial term. Suitable for capturing curvature and nonlinear patterns.\n",
    "\n",
    "3. **Degree of the Polynomial**:\n",
    "   - In polynomial regression, you can choose the degree of the polynomial, which determines the complexity of the model. A quadratic polynomial (\\(X^2\\)) captures curvature, while higher-degree polynomials (\\(X^3\\), \\(X^4\\), etc.) can capture more intricate patterns.\n",
    "\n",
    "4. **Overfitting**:\n",
    "   - Polynomial regression can be prone to overfitting if the degree of the polynomial is too high for the amount of available data. Overfitting occurs when the model fits the training data very closely but generalizes poorly to new, unseen data.\n",
    "\n",
    "5. **Data Transformation**:\n",
    "   - Polynomial regression does not require transformations of the independent variables, as it directly models nonlinear patterns. In contrast, linear regression may require transformations to capture nonlinear relationships.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bbbdda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543b9af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q8. What are the advantages and disadvantages of polynomial regression compared to linear\n",
    "regression? In what situations would you prefer to use polynomial regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7886a1",
   "metadata": {},
   "source": [
    "=>\n",
    "**Advantages of Polynomial Regression**:\n",
    "\n",
    "1. **Captures Nonlinear Relationships**: The primary advantage of polynomial regression is its ability to model and capture nonlinear relationships between the independent and dependent variables. Linear regression, in contrast, is limited to linear relationships.\n",
    "\n",
    "2. **Flexibility**: Polynomial regression is a flexible modeling technique. By adjusting the degree of the polynomial, you can control the level of complexity and fit the model to various data patterns, including curves, bends, and oscillations.\n",
    "\n",
    "3. **Higher Accuracy**: When the underlying relationship between variables is nonlinear, polynomial regression can provide a more accurate representation of the data than linear regression, resulting in improved predictive performance.\n",
    "\n",
    "**Disadvantages of Polynomial Regression**:\n",
    "\n",
    "1. **Overfitting**: Polynomial regression models with high-degree polynomials can be prone to overfitting. When the degree is too high relative to the available data, the model may fit the training data very closely but generalize poorly to new data.\n",
    "\n",
    "2. **Complexity**: Higher-degree polynomial regression models can become complex, with many coefficients to estimate. This complexity can make it more challenging to interpret the model and may require larger datasets.\n",
    "\n",
    "3. **Model Selection**: Selecting the appropriate degree for the polynomial can be challenging. A degree that is too low may not capture the complexity of the data, while a degree that is too high may lead to overfitting.\n",
    "\n",
    "**When to Prefer Polynomial Regression**:\n",
    "\n",
    "You may prefer to use polynomial regression in the following situations:\n",
    "\n",
    "1. **Nonlinear Data Patterns**: When you observe a clear nonlinear relationship between the independent and dependent variables, polynomial regression is a good choice. It can capture curves, bends, and other nonlinear patterns.\n",
    "\n",
    "2. **Data Transformation Limitations**: In some cases, it may be challenging to transform the data to make it linear. Polynomial regression can be a suitable alternative to transforming variables.\n",
    "\n",
    "3. **Simple Data Patterns**: When data exhibits a simple linear relationship, linear regression is typically sufficient. However, if you suspect that a slight curvature or bend exists in the data, polynomial regression with a low-degree polynomial (e.g., quadratic) can be a good choice to account for the subtle nonlinear pattern without introducing too much complexity.\n",
    "\n",
    "4. **Small Datasets**: For small datasets, polynomial regression may be more appropriate than high-complexity models, such as neural networks, which require large datasets. Polynomial regression allows you to capture complex relationships with limited data.\n",
    "\n",
    "5. **Visual Inspection**: If visual inspection of the data suggests a nonlinear pattern, such as a parabolic curve, then polynomial regression may be a reasonable starting point for modeling.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
